{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJBKoaDrpWD-"
   },
   "source": [
    "# Week 06: Collocation Extraction\n",
    "In Assignment 5, we found all skip-grams and their frequencies in <u>*wiki1G.txt*</u>. This week, we want to use the result of assignment 5 to extract collocations of [AKL verbs](https://uclouvain.be/en/research-institutes/ilc/cecl/academic-keyword-list.html). We will use [Smadja’s algorithm](https://aclanthology.org/J93-1007.pdf) to do it. Here are some basic terms need to be explain. \n",
    "\n",
    "We take \"*dpend*\" as an example:\n",
    "\n",
    "<img src=\"https://imgur.com/cPyd7Gr.jpg\" >\n",
    "\n",
    "In this case, we want to find the collocations of \"depend\". Then, \"depend\" is called **base word** and marked as $W$. As for \"on\", \"the\", \"for\"..., they are called **collocate** and marked as $W_{i}$ where **i** represents their serial number. $P_{j}$ means the frequency of $W$ and $W_{i}$ with distance j. And **Freq** is the sum of frequencies of all distances.\n",
    "\n",
    "There are three conditions to filter the skipgram to find collocations. We will go through three conditions below.\n",
    "\n",
    "Considering that some students did not complete Assignment 5, in order to avoid them being unable to do assignment 6, we provide you with a file of calculated skipgram with frequencies, called **AKL_skipgram.tsv**. It only keeps the skipgrams with any AKL verb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBxDYAFPpWEA"
   },
   "source": [
    "## Read Data\n",
    "<font color=\"red\">**[ TODO ]**</font> Please read <u>*AKL_skipgram.tsv*</u> and store it in the way you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1634884174896,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "sVGZSm9fpWEB"
   },
   "outputs": [],
   "source": [
    "#### here are some hyperparameter\n",
    "k0 = 1; k1 = 1; U0 = 10\n",
    "base_word = \"depend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64367,
     "status": "ok",
     "timestamp": 1634886066919,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "ekEseC_PpWEB",
    "outputId": "70168ce0-5074-4cc5-972e-5fe6b560fa83"
   },
   "outputs": [],
   "source": [
    "## read file here\n",
    "data = {}\n",
    "fp = open('AKL_skipgram.tsv')\n",
    "\n",
    "for line in fp.readlines():\n",
    "    tmp_list = line.strip().split('\\t')\n",
    "\n",
    "    for i in range(2, len(tmp_list)):\n",
    "        tmp_list[i] = int(tmp_list[i])\n",
    "    if tmp_list[0] not in data:\n",
    "        data[tmp_list[0]] = [tmp_list[1:len(tmp_list)]]\n",
    "    else:\n",
    "        data[tmp_list[0]].append(tmp_list[1:len(tmp_list)])\n",
    "  #print(tmp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3N9bVOqOpWEB"
   },
   "source": [
    "## C1 Condition\n",
    "C1 helps eliminate the collocates that are not frequent enough. This condition specifies that the frequency of appearance of $W_{i}$ in the neighborhood of $W$ must be at least one standard deviation above the average.\n",
    "\n",
    "The formula is here:\n",
    "\n",
    "$$strength = \\frac{freq - \\bar{f}}{\\sigma} \\geq k_{0} = 1$$\n",
    "\n",
    "where $freq$ is the frequency of certain collocate, (e.g., 2573 for \"on\") and \n",
    "\n",
    "$\\bar{f}$ is the average frequencies of all collocates and \n",
    "\n",
    "${\\sigma}$ is the standard deviation of frequencies of all collocates.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please follow the condition to filter the skipgrams of \"depend\" and keep some which pass the condition.\n",
    "\n",
    "The ouput sholud have `collocate` with its `strength`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1634886299125,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "nQR2AYDxpWEC"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def C1_filter(base_word):\n",
    "    ### [TODO]\n",
    "    C1 = []\n",
    "    words = []\n",
    "    freq = []\n",
    "    for tmp_list in data[base_word]:\n",
    "        words.append(tmp_list[0])\n",
    "        freq.append(tmp_list[1])\n",
    "\n",
    "    mu = numpy.mean(freq); sigma = numpy.std(freq)\n",
    "    \n",
    "    for index in range(len(words)):\n",
    "        \n",
    "        strength = (freq[index] - mu) / sigma\n",
    "        if strength >= k0:\n",
    "            tmp_list = data[base_word][index]\n",
    "            tmp_list.append(round(strength, 3))\n",
    "            C1.append(tmp_list)\n",
    "    \n",
    "    return C1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1634886301568,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "ikso3UZIpWEC",
    "outputId": "2e6b4a96-99f6-4bc0-f8eb-3f69e11a37fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a {'strength':6.381}\n",
      "all {'strength':1.151}\n",
      "also {'strength':1.133}\n",
      "an {'strength':1.367}\n",
      "and {'strength':15.183}\n",
      "are {'strength':1.962}\n",
      "as {'strength':2.395}\n",
      "but {'strength':1.529}\n",
      "by {'strength':1.042}\n",
      "can {'strength':1.421}\n",
      "do {'strength':1.656}\n",
      "does {'strength':5.299}\n",
      "for {'strength':4.686}\n",
      "formula {'strength':1.565}\n",
      "in {'strength':5.876}\n",
      "is {'strength':2.611}\n",
      "it {'strength':2.287}\n",
      "its {'strength':1.818}\n",
      "may {'strength':2.864}\n",
      "not {'strength':8.437}\n",
      "of {'strength':23.461}\n",
      "on {'strength':46.313}\n",
      "only {'strength':1.295}\n",
      "or {'strength':2.485}\n",
      "other {'strength':1.656}\n",
      "properties {'strength':1.042}\n",
      "s {'strength':2.161}\n",
      "some {'strength':1.187}\n",
      "such {'strength':1.439}\n",
      "that {'strength':7.247}\n",
      "the {'strength':44.707}\n",
      "their {'strength':2.828}\n",
      "these {'strength':1.944}\n",
      "they {'strength':2.233}\n",
      "this {'strength':1.908}\n",
      "to {'strength':8.419}\n",
      "type {'strength':1.295}\n",
      "upon {'strength':4.902}\n",
      "which {'strength':4.379}\n",
      "will {'strength':3.784}\n",
      "would {'strength':1.601}\n"
     ]
    }
   ],
   "source": [
    "filtered_by_C1 = C1_filter(base_word)\n",
    "### Print\n",
    "for list_data in filtered_by_C1:\n",
    "    print(list_data[0] + \" {'strength':\" + str(list_data[-1]) + \"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1634883823553,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "Xpo7Pb6SqGmn",
    "outputId": "c890121b-5865-44f5-804a-6c91a6acd518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777.2900000000001\n"
     ]
    }
   ],
   "source": [
    "test = [62, 41, 46, 17, 1, 2, 94, 17, 24, 55]\n",
    "mu = numpy.mean(test)\n",
    "sum = 0\n",
    "for i in test:\n",
    "    sum += (i-mu)**2\n",
    "print(sum/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYguZMfwpWED"
   },
   "source": [
    "<font color=\"green\">Expected output: </font> (The order isn't important.)\n",
    "\n",
    "> a {'strength': 6.381}   \n",
    "> all {'strength': 1.151}   \n",
    "> also {'strength': 1.133}   \n",
    "> an {'strength': 1.367}   \n",
    "> and {'strength': 15.183}   \n",
    "> are {'strength': 1.962}   \n",
    "> as {'strength': 2.395}   \n",
    "> but {'strength': 1.529}   \n",
    "> by {'strength': 1.042}   \n",
    "> can {'strength': 1.421}   \n",
    "> do {'strength': 1.656}   \n",
    "> does {'strength': 5.299}   \n",
    "> for {'strength': 4.686}   \n",
    "> formula {'strength': 1.565}   \n",
    "> in {'strength': 5.876}   \n",
    "> is {'strength': 2.611}   \n",
    "> it {'strength': 2.287}   \n",
    "> its {'strength': 1.818}   \n",
    "> may {'strength': 2.864}   \n",
    "> not {'strength': 8.437}   \n",
    "> of {'strength': 23.461}   \n",
    "> on {'strength': 46.313}   \n",
    "> only {'strength': 1.295}   \n",
    "> or {'strength': 2.485}   \n",
    "> other {'strength': 1.656}   \n",
    "> properties {'strength': 1.042}   \n",
    "> s {'strength': 2.161}   \n",
    "> some {'strength': 1.187}   \n",
    "> such {'strength': 1.439}   \n",
    "> that {'strength': 7.247}   \n",
    "> the {'strength': 44.707}   \n",
    "> their {'strength': 2.828}   \n",
    "> these {'strength': 1.944}   \n",
    "> they {'strength': 2.233}   \n",
    "> this {'strength': 1.908}   \n",
    "> to {'strength': 8.419}   \n",
    "> type {'strength': 1.295}   \n",
    "> upon {'strength': 4.902}   \n",
    "> which {'strength': 4.379}   \n",
    "> will {'strength': 3.784}   \n",
    "> would {'strength': 1.601}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lWRmQrGpWED"
   },
   "source": [
    "## C2 Condition\n",
    "C2 requires that the histogram of the 10 relative frequencies of appearance of $W_i$ within five words of $W$ (or $p^j_i$s) have at least one spike. If the histogram is flat, it will be rejected by this condition.\n",
    "\n",
    "The formula is here:\n",
    "\n",
    "$$spread = \\frac{\\Sigma^{10}_{j=1}(p^j_i - \\bar{p_i})^2}{10} \\geq U_{0} = 10$$\n",
    "\n",
    "where $p^j_i$ is the frequency of certain collocate with a distance of *j*, (e.g., 16 for \"on\" when its distance is -5) and \n",
    "\n",
    "$\\bar{p_i}$ is the average frequencies of \"on\" with any distance \n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please follow C2 to filter the result of C1 and keep some which pass C2.\n",
    "\n",
    "The ouput sholud have `collocate` with `strength` and `spread`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1634886307651,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "s2p0LbZtpWEE"
   },
   "outputs": [],
   "source": [
    "def C2_filter(base_word, filtered_by_C1):\n",
    "    ### [TODO]\n",
    "    C2 = []\n",
    "    for data in filtered_by_C1:\n",
    "        tmp_data = data[2:-2]\n",
    "        mu = numpy.mean(tmp_data)\n",
    "        spread = 0\n",
    "        for i in tmp_data:\n",
    "            spread += (i - mu) ** 2\n",
    "        spread = round(spread / 10, 2)\n",
    "        if spread >= U0:\n",
    "            tmp_list = data\n",
    "            tmp_list.append(spread)\n",
    "            C2.append(tmp_list)\n",
    "            \n",
    "    return C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1634886309691,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "OJPDVL0WpWEE",
    "outputId": "b6b7793a-9866-4dc3-9339-97241a8cdf45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a {'strength':6.381, 'spread':736.76}\n",
      "all {'strength':1.151, 'spread':27.22}\n",
      "also {'strength':1.133, 'spread':207.36}\n",
      "an {'strength':1.367, 'spread':43.09}\n",
      "and {'strength':15.183, 'spread':2162.76}\n",
      "are {'strength':1.962, 'spread':90.62}\n",
      "as {'strength':2.395, 'spread':82.56}\n",
      "but {'strength':1.529, 'spread':20.4}\n",
      "by {'strength':1.042, 'spread':25.4}\n",
      "can {'strength':1.421, 'spread':207.6}\n",
      "do {'strength':1.656, 'spread':406.6}\n",
      "does {'strength':5.299, 'spread':6384.29}\n",
      "for {'strength':4.686, 'spread':342.62}\n",
      "formula {'strength':1.565, 'spread':35.49}\n",
      "in {'strength':5.876, 'spread':382.89}\n",
      "is {'strength':2.611, 'spread':126.42}\n",
      "it {'strength':2.287, 'spread':112.76}\n",
      "its {'strength':1.818, 'spread':91.0}\n",
      "may {'strength':2.864, 'spread':1349.0}\n",
      "not {'strength':8.437, 'spread':12766.8}\n",
      "of {'strength':23.461, 'spread':19932.89}\n",
      "on {'strength':46.313, 'spread':414426.6}\n",
      "only {'strength':1.295, 'spread':133.69}\n",
      "or {'strength':2.485, 'spread':72.89}\n",
      "other {'strength':1.656, 'spread':25.69}\n",
      "properties {'strength':1.042, 'spread':28.16}\n",
      "s {'strength':2.161, 'spread':125.82}\n",
      "some {'strength':1.187, 'spread':15.16}\n",
      "such {'strength':1.439, 'spread':25.2}\n",
      "that {'strength':7.247, 'spread':1457.8}\n",
      "the {'strength':44.707, 'spread':97177.29}\n",
      "their {'strength':2.828, 'spread':196.6}\n",
      "these {'strength':1.944, 'spread':175.6}\n",
      "they {'strength':2.233, 'spread':307.29}\n",
      "this {'strength':1.908, 'spread':69.4}\n",
      "to {'strength':8.419, 'spread':3936.89}\n",
      "type {'strength':1.295, 'spread':213.09}\n",
      "upon {'strength':4.902, 'spread':4904.8}\n",
      "which {'strength':4.379, 'spread':345.62}\n",
      "will {'strength':3.784, 'spread':2198.69}\n",
      "would {'strength':1.601, 'spread':407.89}\n"
     ]
    }
   ],
   "source": [
    "filtered_by_C2 = C2_filter(base_word, filtered_by_C1)\n",
    "### Print\n",
    "for list_data in filtered_by_C2:\n",
    "    print(list_data[0] + \" {'strength':\" + str(list_data[-2]) + \", 'spread':\" + str(list_data[-1]) + \"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blC4RZwwpWEE"
   },
   "source": [
    "<font color=\"green\">Expected output: </font> (The order isn't important.)\n",
    "\n",
    "> a {'strength': 6.381, 'spread': 777.29}   \n",
    "> all {'strength': 1.151, 'spread': 29.89}   \n",
    "> also {'strength': 1.133, 'spread': 208.96}   \n",
    "> an {'strength': 1.367, 'spread': 56.29}   \n",
    "> and {'strength': 15.183, 'spread': 2170.41}   \n",
    "> are {'strength': 1.962, 'spread': 98.84}   \n",
    "> as {'strength': 2.395, 'spread': 104.96}   \n",
    "> but {'strength': 1.529, 'spread': 24.4}   \n",
    "> by {'strength': 1.042, 'spread': 26.21}   \n",
    "> can {'strength': 1.421, 'spread': 208.24}   \n",
    "> do {'strength': 1.656, 'spread': 410.21}   \n",
    "> does {'strength': 5.299, 'spread': 6477.09}   \n",
    "> for {'strength': 4.686, 'spread': 376.65}   \n",
    "> formula {'strength': 1.565, 'spread': 46.16}   \n",
    "> in {'strength': 5.876, 'spread': 396.09}   \n",
    "> is {'strength': 2.611, 'spread': 148.2}   \n",
    "> it {'strength': 2.287, 'spread': 112.76}   \n",
    "> its {'strength': 1.818, 'spread': 94.24}   \n",
    "> may {'strength': 2.864, 'spread': 1352.24}   \n",
    "> not {'strength': 8.437, 'spread': 12938.41}   \n",
    "> of {'strength': 23.461, 'spread': 20132.64}   \n",
    "> on {'strength': 46.313, 'spread': 420371.01}   \n",
    "> only {'strength': 1.295, 'spread': 134.01}   \n",
    "> or {'strength': 2.485, 'spread': 85.61}   \n",
    "> other {'strength': 1.656, 'spread': 31.61}   \n",
    "> properties {'strength': 1.042, 'spread': 30.21}   \n",
    "> s {'strength': 2.161, 'spread': 125.85}   \n",
    "> some {'strength': 1.187, 'spread': 15.29}   \n",
    "> such {'strength': 1.439, 'spread': 27.45}   \n",
    "> that {'strength': 7.247, 'spread': 1492.61}   \n",
    "> the {'strength': 44.707, 'spread': 98586.04}   \n",
    "> their {'strength': 2.828, 'spread': 209.56}   \n",
    "> these {'strength': 1.944, 'spread': 180.01}   \n",
    "> they {'strength': 2.233, 'spread': 316.09}   \n",
    "> this {'strength': 1.908, 'spread': 71.09}   \n",
    "> to {'strength': 8.419, 'spread': 3941.16}   \n",
    "> type {'strength': 1.295, 'spread': 213.41}   \n",
    "> upon {'strength': 4.902, 'spread': 4984.01}   \n",
    "> which {'strength': 4.379, 'spread': 346.16}   \n",
    "> will {'strength': 3.784, 'spread': 2250.05}   \n",
    "> would {'strength': 1.601, 'spread': 412.44}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naS9EpYZpWEF"
   },
   "source": [
    "## C3 Condition\n",
    "C3 keeps the interesting collocates by pulling out the peaks of the $p^j_i$ distributions.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$p^j_i \\geq \\bar{p_i} + (k_1 \\times \\sqrt{U_{i}})$$\n",
    "\n",
    "where $U_i$ is *spread* in C2 and\n",
    "\n",
    "$k_1$ is equal to 1 \n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please follow the condition to filter the result of last step and keep some which pass C3.\n",
    "\n",
    "The ouput sholud have `base word, collocate, distance, strength, spread, peak, count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1634887742115,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "1aHJ7GHlpWEF"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def C3_filter(base_word, filtered_by_C2):\n",
    "    ### [TODO]\n",
    "    C3 = []\n",
    "    for tmp_data in filtered_by_C2:\n",
    "        tmp_freq = tmp_data[2:-3]\n",
    "        mu = numpy.mean(tmp_freq)\n",
    "        threshold = mu + k1 * math.sqrt(tmp_data[-1])\n",
    "        for freq in tmp_freq:\n",
    "            if freq > threshold:\n",
    "                index = tmp_freq.index(freq)\n",
    "                distance = index - 5 if index < 5 else index - 4\n",
    "                tmp = [base_word, tmp_data[0], distance, tmp_data[-2], tmp_data[-1], round(threshold, 3), freq]\n",
    "                C3.append(tmp)\n",
    "    return C3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1634887757064,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "bFNox2TYpWEF",
    "outputId": "516d9b29-334a-41d3-f86b-245ed259d689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('depend', 'a', -5) {'strength': 6.381, 'spread': 736.76, 'peak': 60.921, 'count': 62}\n",
      "('depend', 'a', 2) {'strength': 6.381, 'spread': 736.76, 'peak': 60.921, 'count': 94}\n",
      "('depend', 'all', -4) {'strength': 1.151, 'spread': 27.22, 'peak': 12.662, 'count': 14}\n",
      "('depend', 'all', -3) {'strength': 1.151, 'spread': 27.22, 'peak': 12.662, 'count': 16}\n",
      "('depend', 'also', -1) {'strength': 1.133, 'spread': 207.36, 'peak': 21.622, 'count': 50}\n",
      "('depend', 'an', 2) {'strength': 1.367, 'spread': 43.09, 'peak': 13.453, 'count': 24}\n",
      "('depend', 'and', 4) {'strength': 15.183, 'spread': 2162.76, 'peak': 130.283, 'count': 149}\n",
      "('depend', 'are', -5) {'strength': 1.962, 'spread': 90.62, 'peak': 19.964, 'count': 27}\n",
      "('depend', 'are', -4) {'strength': 1.962, 'spread': 90.62, 'peak': 19.964, 'count': 22}\n",
      "('depend', 'are', -3) {'strength': 1.962, 'spread': 90.62, 'peak': 19.964, 'count': 20}\n",
      "('depend', 'as', 4) {'strength': 2.395, 'spread': 82.56, 'peak': 21.308, 'count': 30}\n",
      "('depend', 'but', -2) {'strength': 1.529, 'spread': 20.4, 'peak': 12.85, 'count': 14}\n",
      "('depend', 'by', -5) {'strength': 1.042, 'spread': 25.4, 'peak': 11.04, 'count': 13}\n",
      "('depend', 'by', -4) {'strength': 1.042, 'spread': 25.4, 'peak': 11.04, 'count': 12}\n",
      "('depend', 'by', -5) {'strength': 1.042, 'spread': 25.4, 'peak': 11.04, 'count': 13}\n",
      "('depend', 'can', -1) {'strength': 1.421, 'spread': 207.6, 'peak': 23.075, 'count': 49}\n",
      "('depend', 'do', -2) {'strength': 1.656, 'spread': 406.6, 'peak': 30.498, 'count': 70}\n",
      "('depend', 'does', -2) {'strength': 5.299, 'spread': 6384.29, 'peak': 113.013, 'count': 271}\n",
      "('depend', 'for', 4) {'strength': 4.686, 'spread': 342.62, 'peak': 43.066, 'count': 69}\n",
      "('depend', 'formula', -4) {'strength': 1.565, 'spread': 35.49, 'peak': 14.068, 'count': 19}\n",
      "('depend', 'formula', 2) {'strength': 1.565, 'spread': 35.49, 'peak': 14.068, 'count': 17}\n",
      "('depend', 'in', -5) {'strength': 5.876, 'spread': 382.89, 'peak': 51.456, 'count': 55}\n",
      "('depend', 'in', 4) {'strength': 5.876, 'spread': 382.89, 'peak': 51.456, 'count': 62}\n",
      "('depend', 'is', -5) {'strength': 2.611, 'spread': 126.42, 'peak': 24.688, 'count': 37}\n",
      "('depend', 'it', -3) {'strength': 2.287, 'spread': 112.76, 'peak': 23.841, 'count': 39}\n",
      "('depend', 'it', -2) {'strength': 2.287, 'spread': 112.76, 'peak': 23.841, 'count': 24}\n",
      "('depend', 'its', 2) {'strength': 1.818, 'spread': 91.0, 'peak': 19.539, 'count': 36}\n",
      "('depend', 'may', -1) {'strength': 2.864, 'spread': 1349.0, 'peak': 53.729, 'count': 126}\n",
      "('depend', 'not', -1) {'strength': 8.437, 'spread': 12766.8, 'peak': 164.657, 'count': 388}\n",
      "('depend', 'of', 4) {'strength': 23.461, 'spread': 19932.89, 'peak': 267.073, 'count': 495}\n",
      "('depend', 'on', 1) {'strength': 46.313, 'spread': 414426.6, 'peak': 926.76, 'count': 2195}\n",
      "('depend', 'only', 1) {'strength': 1.295, 'spread': 133.69, 'peak': 19.451, 'count': 40}\n",
      "('depend', 'or', 4) {'strength': 2.485, 'spread': 72.89, 'peak': 21.649, 'count': 29}\n",
      "('depend', 'other', 3) {'strength': 1.656, 'spread': 25.69, 'peak': 13.957, 'count': 19}\n",
      "('depend', 'properties', -1) {'strength': 1.042, 'spread': 28.16, 'peak': 12.084, 'count': 15}\n",
      "('depend', 'properties', -1) {'strength': 1.042, 'spread': 28.16, 'peak': 12.084, 'count': 15}\n",
      "('depend', 's', 4) {'strength': 2.161, 'spread': 125.82, 'peak': 23.661, 'count': 41}\n",
      "('depend', 'some', -3) {'strength': 1.187, 'spread': 15.16, 'peak': 11.116, 'count': 13}\n",
      "('depend', 'some', 2) {'strength': 1.187, 'spread': 15.16, 'peak': 11.116, 'count': 14}\n",
      "('depend', 'such', 4) {'strength': 1.439, 'spread': 25.2, 'peak': 13.02, 'count': 17}\n",
      "('depend', 'that', -3) {'strength': 7.247, 'spread': 1457.8, 'peak': 80.848, 'count': 84}\n",
      "('depend', 'that', -1) {'strength': 7.247, 'spread': 1457.8, 'peak': 80.848, 'count': 132}\n",
      "('depend', 'the', 2) {'strength': 44.707, 'spread': 97177.29, 'peak': 547.622, 'count': 1140}\n",
      "('depend', 'their', 2) {'strength': 2.828, 'spread': 196.6, 'peak': 29.021, 'count': 52}\n",
      "('depend', 'these', -2) {'strength': 1.944, 'spread': 175.6, 'peak': 25.251, 'count': 48}\n",
      "('depend', 'they', -1) {'strength': 2.233, 'spread': 307.29, 'peak': 31.419, 'count': 63}\n",
      "('depend', 'this', -4) {'strength': 1.908, 'spread': 69.4, 'peak': 18.997, 'count': 28}\n",
      "('depend', 'this', -2) {'strength': 1.908, 'spread': 69.4, 'peak': 18.997, 'count': 22}\n",
      "('depend', 'to', -1) {'strength': 8.419, 'spread': 3936.89, 'peak': 110.634, 'count': 228}\n",
      "('depend', 'type', 3) {'strength': 1.295, 'spread': 213.09, 'peak': 22.486, 'count': 50}\n",
      "('depend', 'upon', 1) {'strength': 4.902, 'spread': 4904.8, 'peak': 100.701, 'count': 239}\n",
      "('depend', 'which', -1) {'strength': 4.379, 'spread': 345.62, 'peak': 43.146, 'count': 66}\n",
      "('depend', 'will', -1) {'strength': 3.784, 'spread': 2198.69, 'peak': 70.779, 'count': 159}\n",
      "('depend', 'would', -1) {'strength': 1.601, 'spread': 407.89, 'peak': 30.307, 'count': 70}\n"
     ]
    }
   ],
   "source": [
    "filtered_by_C3 = C3_filter(base_word, filtered_by_C2)\n",
    "### Print\n",
    "for tmp_list in filtered_by_C3:\n",
    "    print(\"('\" + tmp_list[0] + \"', '\" + tmp_list[1] + \"', \" + str(tmp_list[2]) + \") {'strength': \" + str(tmp_list[3]) + \", 'spread': \" + str(tmp_list[4]) + \", 'peak': \" + str(tmp_list[5]) + \", 'count': \" + str(tmp_list[6]) + \"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K43nWOqpWEF"
   },
   "source": [
    "<font color=\"green\">Expected output: </font> (The order isn't important.)\n",
    "\n",
    "> ('depend', 'a', 2) {'strength': 6.381, 'spread': 777.29, 'peak': 63.78, 'count': 94}   \n",
    "> ('depend', 'all', -4) {'strength': 1.151, 'spread': 29.89, 'peak': 12.367, 'count': 14}   \n",
    "> ('depend', 'all', -3) {'strength': 1.151, 'spread': 29.89, 'peak': 12.367, 'count': 16}   \n",
    "> ('depend', 'also', -1) {'strength': 1.133, 'spread': 208.96, 'peak': 21.255, 'count': 50}   \n",
    "> ('depend', 'an', 2) {'strength': 1.367, 'spread': 56.29, 'peak': 15.603, 'count': 24}   \n",
    "> ('depend', 'an', 5) {'strength': 1.367, 'spread': 56.29, 'peak': 15.603, 'count': 19}   \n",
    "> ('depend', 'and', 4) {'strength': 15.183, 'spread': 2170.41, 'peak': 131.288, 'count': 149}   \n",
    "> ('depend', 'are', -5) {'strength': 1.962, 'spread': 98.84, 'peak': 21.342, 'count': 27}   \n",
    "> ('depend', 'are', -4) {'strength': 1.962, 'spread': 98.84, 'peak': 21.342, 'count': 22}   \n",
    "> ('depend', 'as', 4) {'strength': 2.395, 'spread': 104.96, 'peak': 24.045, 'count': 30}   \n",
    "> ('depend', 'as', 5) {'strength': 2.395, 'spread': 104.96, 'peak': 24.045, 'count': 28}   \n",
    "> ('depend', 'but', -2) {'strength': 1.529, 'spread': 24.4, 'peak': 13.94, 'count': 14}   \n",
    "> ('depend', 'but', 5) {'strength': 1.529, 'spread': 24.4, 'peak': 13.94, 'count': 15}   \n",
    "> ('depend', 'by', -5) {'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 13}   \n",
    "> ('depend', 'by', -4) {'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 12}   \n",
    "> ('depend', 'by', 4) {'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 13}   \n",
    "> ('depend', 'can', -1) {'strength': 1.421, 'spread': 208.24, 'peak': 22.831, 'count': 49}   \n",
    "> ('depend', 'do', -2) {'strength': 1.656, 'spread': 410.21, 'peak': 29.954, 'count': 70}   \n",
    "> ('depend', 'does', -2) {'strength': 5.299, 'spread': 6477.09, 'peak': 110.38, 'count': 271}   \n",
    "> ('depend', 'for', 4) {'strength': 4.686, 'spread': 376.65, 'peak': 45.907, 'count': 69}   \n",
    "> ('depend', 'formula', -4) {'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 19}   \n",
    "> ('depend', 'formula', 2) {'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 17}   \n",
    "> ('depend', 'formula', 5) {'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 19}   \n",
    "> ('depend', 'in', -5) {'strength': 5.876, 'spread': 396.09, 'peak': 53.002, 'count': 55}   \n",
    "> ('depend', 'in', 4) {'strength': 5.876, 'spread': 396.09, 'peak': 53.002, 'count': 62}   \n",
    "> ('depend', 'is', -5) {'strength': 2.611, 'spread': 148.2, 'peak': 27.174, 'count': 37}   \n",
    "> ('depend', 'is', 5) {'strength': 2.611, 'spread': 148.2, 'peak': 27.174, 'count': 29}   \n",
    "> ('depend', 'it', -3) {'strength': 2.287, 'spread': 112.76, 'peak': 23.819, 'count': 39}   \n",
    "> ('depend', 'it', -2) {'strength': 2.287, 'spread': 112.76, 'peak': 23.819, 'count': 24}   \n",
    "> ('depend', 'its', 2) {'strength': 1.818, 'spread': 94.24, 'peak': 20.308, 'count': 36}   \n",
    "> ('depend', 'may', -1) {'strength': 2.864, 'spread': 1352.24, 'peak': 53.173, 'count': 126}   \n",
    "> ('depend', 'not', -1) {'strength': 8.437, 'spread': 12938.41, 'peak': 161.047, 'count': 388}   \n",
    "> ('depend', 'of', 4) {'strength': 23.461, 'spread': 20132.64, 'peak': 272.49, 'count': 495}   \n",
    "> ('depend', 'on', 1) {'strength': 46.313, 'spread': 420371.01, 'peak': 905.66, 'count': 2195}   \n",
    "> ('depend', 'only', 1) {'strength': 1.295, 'spread': 134.01, 'peak': 19.276, 'count': 40}   \n",
    "> ('depend', 'or', 4) {'strength': 2.485, 'spread': 85.61, 'peak': 23.553, 'count': 29}   \n",
    "> ('depend', 'or', 5) {'strength': 2.485, 'spread': 85.61, 'peak': 23.553, 'count': 25}   \n",
    "> ('depend', 'other', 3) {'strength': 1.656, 'spread': 31.61, 'peak': 15.322, 'count': 19}   \n",
    "> ('depend', 'other', 5) {'strength': 1.656, 'spread': 31.61, 'peak': 15.322, 'count': 17}   \n",
    "> ('depend', 'properties', -4) {'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 12}   \n",
    "> ('depend', 'properties', -1) {'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 15}   \n",
    "> ('depend', 'properties', 3) {'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 15}   \n",
    "> ('depend', 's', 4) {'strength': 2.161, 'spread': 125.85, 'peak': 23.718, 'count': 41}   \n",
    "> ('depend', 'some', -3) {'strength': 1.187, 'spread': 15.29, 'peak': 11.01, 'count': 13}   \n",
    "> ('depend', 'some', 2) {'strength': 1.187, 'spread': 15.29, 'peak': 11.01, 'count': 14}   \n",
    "> ('depend', 'such', 4) {'strength': 1.439, 'spread': 27.45, 'peak': 13.739, 'count': 17}   \n",
    "> ('depend', 'that', -3) {'strength': 7.247, 'spread': 1492.61, 'peak': 79.334, 'count': 84}   \n",
    "> ('depend', 'that', -1) {'strength': 7.247, 'spread': 1492.61, 'peak': 79.334, 'count': 132}   \n",
    "> ('depend', 'the', 2) {'strength': 44.707, 'spread': 98586.04, 'peak': 562.384, 'count': 1140}   \n",
    "> ('depend', 'their', 2) {'strength': 2.828, 'spread': 209.56, 'peak': 30.676, 'count': 52}   \n",
    "> ('depend', 'these', -2) {'strength': 1.944, 'spread': 180.01, 'peak': 24.717, 'count': 48}   \n",
    "> ('depend', 'they', -1) {'strength': 2.233, 'spread': 316.09, 'peak': 30.679, 'count': 63}   \n",
    "> ('depend', 'this', -4) {'strength': 1.908, 'spread': 71.09, 'peak': 19.531, 'count': 28}   \n",
    "> ('depend', 'this', -2) {'strength': 1.908, 'spread': 71.09, 'peak': 19.531, 'count': 22}   \n",
    "> ('depend', 'to', -1) {'strength': 8.419, 'spread': 3941.16, 'peak': 109.979, 'count': 228}   \n",
    "> ('depend', 'type', 3) {'strength': 1.295, 'spread': 213.41, 'peak': 22.309, 'count': 50}   \n",
    "> ('depend', 'upon', 1) {'strength': 4.902, 'spread': 4984.01, 'peak': 98.298, 'count': 239}   \n",
    "> ('depend', 'which', -1) {'strength': 4.379, 'spread': 346.16, 'peak': 43.405, 'count': 66}   \n",
    "> ('depend', 'will', -1) {'strength': 3.784, 'spread': 2250.05, 'peak': 68.935, 'count': 159}   \n",
    "> ('depend', 'would', -1) {'strength': 1.601, 'spread': 412.44, 'peak': 29.709, 'count': 70}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bMqATOZpWEG"
   },
   "source": [
    "## Strongest Collocation\n",
    "There are too many collocations to check your result easily. Hence, we want you use the rules below to find out one strongest collocation for \"depend\".\n",
    "\n",
    "Rule:\n",
    "1. find the collocate with maximum **`strength`** value\n",
    "2. find the collocate with maximum **`count`** value\n",
    "\n",
    "If there're more than two collocations sharing same maximum `strength` value, please use rule 2 to find one as the answer. Otherwise, you can ignore Rule 2.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please find out the strongest collocation for \"depend\" by the rules.\n",
    "\n",
    "The ouput format sholud be `(base word, collocate, distance)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1634888581822,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "idhTENjepWEG"
   },
   "outputs": [],
   "source": [
    "def find_strongest_collocation(base_word, filtered_by_C3):\n",
    "    ### [TODO]\n",
    "    max_strength = 0\n",
    "    max_count = 0\n",
    "\n",
    "    max_word = ''\n",
    "    max_distance = 0\n",
    "  \n",
    "    for tmp_list in filtered_by_C3:\n",
    "        if tmp_list[3] > max_strength:\n",
    "            max_strength = tmp_list[3]\n",
    "            max_count = tmp_list[6]\n",
    "            \n",
    "            max_word = tmp_list[1]\n",
    "            max_distance = tmp_list[2]\n",
    "\n",
    "        elif tmp_list[3] == max_strength:\n",
    "            if tmp_list[6] > max_count:\n",
    "                max_count = tmp_list[6]\n",
    "                max_word = tmp_list[1]\n",
    "                max_distance = tmp_list[2]\n",
    "    print(\"('\" + base_word + \"', '\" + max_word + \"', \" + str(max_distance) + \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1634888583272,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "dPxhbWqYpWEG",
    "outputId": "e6fc841c-dd2f-455f-c6dc-2adf2a650fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('depend', 'on', 1)\n"
     ]
    }
   ],
   "source": [
    "### Run and Print\n",
    "find_strongest_collocation(base_word, filtered_by_C3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQFst4LLpWEG"
   },
   "source": [
    "<font color=\"green\">Expected output: </font>\n",
    "\n",
    "> ('depend', 'on', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ub8AdaebpWEG"
   },
   "source": [
    "## Find Helpful AKL Collocation\n",
    "Only one example cannot express how amazing what we just did, so here are some other AKL verbs selected for you to experience. \n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please finish **combination** function to combine last four functions together and use it to find out strongest collocations for **AKL_verbs**. \n",
    "\n",
    "The ouput format sholud be `(base word, collocate, distance)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1634888817473,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "fPZ6ulIJpWEG"
   },
   "outputs": [],
   "source": [
    "AKL_verbs = ['argue', 'can', 'consist', 'contrast', 'favour', 'lack', 'may', \n",
    "            'neglect', 'participate', 'present', 'rely', 'suggest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1634888818650,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "Yolxm1hfpWEG"
   },
   "outputs": [],
   "source": [
    "def combination(base_word):\n",
    "### [TODO]\n",
    "    filtered_by_C1 = C1_filter(base_word)\n",
    "    filtered_by_C2 = C2_filter(base_word, filtered_by_C1)\n",
    "    filtered_by_C3 = C3_filter(base_word, filtered_by_C2)\n",
    "    find_strongest_collocation(base_word, filtered_by_C3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1634888820596,
     "user": {
      "displayName": "林柏全",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14743617104411089585"
     },
     "user_tz": -480
    },
    "id": "31okIDBMpWEG",
    "outputId": "721dafc1-3176-4c68-e6b4-a0a311c581d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('argue', 'that', 1)\n",
      "('can', 'be', 1)\n",
      "('consist', 'of', 1)\n",
      "('contrast', 'in', -1)\n",
      "('favour', 'of', 1)\n",
      "('lack', 'of', 1)\n",
      "('may', 'be', 1)\n",
      "('neglect', 'of', 1)\n",
      "('participate', 'in', 1)\n",
      "('present', 'with', -3)\n",
      "('rely', 'on', 1)\n",
      "('suggest', 'that', 1)\n"
     ]
    }
   ],
   "source": [
    "### Run and Print\n",
    "for word in AKL_verbs:\n",
    "    combination(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-laz0HgapWEH"
   },
   "source": [
    "# <font color=\"green\">Expected output: </font>\n",
    "\n",
    "> ('argue', 'that', 1)   \n",
    "> ('can', 'be', 1)   \n",
    "> ('consist', 'of', 1)   \n",
    "> ('contrast', 'in', -1)   \n",
    "> ('favour', 'of', 1)   \n",
    "> ('lack', 'of', 1)   \n",
    "> ('may', 'be', 1)   \n",
    "> ('neglect', 'of', 1)   \n",
    "> ('participate', 'in', 1)   \n",
    "> ('present', 'with', -3)   \n",
    "> ('rely', 'on', 1)   \n",
    "> ('suggest', 'that', 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO64hc-OpWEH"
   },
   "source": [
    "## TA's Notes\n",
    "\n",
    "If you complete the Assignment, please use [this link](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit#gid=206119035) to reserve demo time.  \n",
    "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with a TA before you miss the deadline**</u> .  <br>After demo, please upload your assignment to eeclass. You just need to hand in this ipynb file and rename it as XXXXXXXXX(Your student ID).ipynb.\n",
    "<br>Note that **late submission will not be allowed**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oayUKgdNpWEH"
   },
   "source": [
    "## Reference\n",
    "[Frank Smadja, Retrieving Collocations from Texts: Xtract, Computational Linguistics, Volume 19, 1993](https://aclanthology.org/J93-1007.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
