{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 03: Simple Language Model\n",
    "\n",
    "This week, we want you to create a Language Model (LM) to judge how common a sentence is.  \n",
    "More specificly, you need to <u>calculate the probability of a given setnence</u> showing in an article.  \n",
    "\n",
    "*No need to make your output exactly the same as ours. As long as it's *reasonable* (which means you can explain it), you get your points.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Language Model?\n",
    "\n",
    "<b>A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length $m$, it assigns a probability $P(w_1, \\ldots ,w_m)$ to the whole sequence.</b>\n",
    "<i>-- from <a href=\"https://en.wikipedia.org/wiki/Language_model\">Wikipedia</a></i>  \n",
    "\n",
    "To put it simply, Language Model calculates the probability of a word, a sequence of words, or a sentence.  \n",
    "This can be useful in many NLP tasks, like machine translation, spelling checking, speech recognition, etc.  \n",
    "\n",
    "Consider the following two sentence  \n",
    "\n",
    "> (1) He is looking <font color=\"green\">*to*</font> a new job.  \n",
    "\n",
    "and \n",
    "\n",
    "> (2) He is looking <font color=\"green\">*for*</font> a new job.  \n",
    "\n",
    "\n",
    "We can see that there's a grammar error in sentence 1, for which the LM should generate a lower probability.  \n",
    "Hence, if your LM is well-established, you are able to judge which sentence is more correct (or more common) among a set of candidate sentences.  \n",
    "\n",
    "But how to do so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Step 1 - Ngram frequency\n",
    "\n",
    "First of all, we need to calculate 1- and 2-gram frequency from the corpus.  \n",
    "**Again**, yes. As we have said in the first class, word/phrase frequency plays an important role in NLP.\n",
    "\n",
    "You should be familiar with this procedure now, so let's skip the boring explanation.  \n",
    "<small>*Please refer to assignment 1 and 2 if you have any question.</small>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "with open(os.path.join('data', 'big.txt'), 'r') as f:\n",
    "  corpus = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Calculate 1- and 2-gram frequency with the given corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram(text):\n",
    "    \"\"\" [TODO] Get the 1-gram list from string\n",
    "    @param text: given string\n",
    "    @return: a list of token\n",
    "    \"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "def bigram(text):\n",
    "    \"\"\" [TODO] Get the 2-gram list \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    n_grams = ngrams(tokens, 2)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# [TODO] calculate the 1-gram frequency of the corpus\n",
    "uni = unigram(corpus)\n",
    "uni_freq = Counter(uni)\n",
    "# [TODO] calculate the 2-gram frequency of the corpus\n",
    "bi = bigram(corpus)\n",
    "bi_freq = Counter(bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Probability of a word\n",
    "\n",
    "#### Unigram  \n",
    "Probability of a word, or a *unigram*, is simple: its occurence devided by the total word count should do the trick.  \n",
    "$$P(w) = \\frac{Freq(w)}{N}$$  \n",
    ", where $N = $ count of all words.  \n",
    "But the unigram language model doesn't consider other context words, so we choose to use bigram model here.  \n",
    "\n",
    "#### Bigram  \n",
    "As to 2-gram probability, we would use the Maximum Likelihood Estimate (MLE) to calculate it.  \n",
    "\n",
    "$$P_{mle}(w_i|w_{i-1}) = \\frac{Freq(w_{i-1}, w_i)}{Freq(w_{i-1})}$$\n",
    "\n",
    "For example, if we have a corpus with \n",
    "\n",
    "> can you tell me about any good cantonese restaurants  \n",
    "> tell me about chez panisse  \n",
    "> can you give me a listing of the kinds of food that are available  \n",
    "\n",
    "Then the probability of *tell* preceding *me* is $P_{mle}(me|tell) = \\frac{2}{2} = 1$.  \n",
    "Similarly, the probability of *me* preceding *about* is $P_{mle}(about|me) = \\frac{2}{3}$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Calculate the bigram probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_mle(word: str, pre_word: str = None):\n",
    "    \"\"\" [TODO] Return the P(word|pre_word), which should be a float \"\"\"\n",
    "    text = pre_word+' '+word\n",
    "    return bi_freq[text]/uni_freq[pre_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 0.0; \n",
      "p2: 0.005988023952095809\n"
     ]
    }
   ],
   "source": [
    "p1 = P_mle('book', 'read')\n",
    "p2 = P_mle('books', 'read')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2` (expetced to be `0`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 0.0; \n",
      "p2: 0.0004793608521970706\n"
     ]
    }
   ],
   "source": [
    "p1 = P_mle('books', 'a')\n",
    "p2 = P_mle('book', 'a')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2` (expetced to be `0`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 0.0004998000799680128; \n",
      "p2: 0.014594162335065974\n"
     ]
    }
   ],
   "source": [
    "p1 = P_mle('have', 'he')\n",
    "p2 = P_mle('has', 'he')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Sentence probability\n",
    "\n",
    "Now we have the probability of every word and bigram. But how should we combine them and get the probability of a sentence?  \n",
    "\n",
    "Recall the chain rule in probability: \n",
    "$$P(X_1, X_2, X_3, X_4) = P(X_1) \\cdot P(X_2|X_1) \\cdot P(X_3|X_1,X_2) \\cdot P(X_4|X_1, X_2, X_3)$$  \n",
    "And with Markove Assumption, we know that \n",
    "$$P(w_1, w_2, \\dots, w_n) \\approx P(w_2|w_1) \\cdot P(w_3|w_2) \\cdot \\dots \\cdot P(w_n|w_{n-1})$$\n",
    "\n",
    "For example,\n",
    "$$P(I\\:want\\:a\\:cake) \\approx P(want|I) \\cdot P(a|want) \\cdot P(cake|a)$$\n",
    "\n",
    "Since we have already built a method to calculate $P(w_{i}|w_{i-1})$, we can get the probability of a sentence through this equation.  \n",
    "Note that it's recommended to sum them under $log$-scale to prevent underflow, because gram probabilities are mostly some small floating points, \n",
    "$$log(p_1 \\cdot p_2 \\dots p_n) = log(p_1) + log(p_2) + \\dots + log(p_n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Calculate the sentence probability. (Please do so under the `log` scale to prevent underflow)  \n",
    "<small>*Hint: `math`</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'have', 'to', 'take', 'the', 'medicine.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram('He have to take the medicine.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def sentence_prob(sentence: str, P):\n",
    "    \"\"\" Calculate the probability of a given sentence with given P-model.\n",
    "    @param sentence: the given sentence\n",
    "    @param P: the probability model to use. (like your `P_mle` above)\n",
    "    @return: a float indicating the probability\n",
    "    \"\"\"\n",
    "    words = unigram(sentence)\n",
    "    prob = 0\n",
    "    for i in range(0, len(words)-1):\n",
    "        prob += log(P(words[i+1], words[i]))\n",
    "        #print(words[i+1] + ' | ' + words[i])\n",
    "        #print(log(P_mle(words[i+1], words[i])))\n",
    "    ... # [TODO] calculate the probability\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: -29.30948724296647; \n",
      "p2: -26.62036562628031\n"
     ]
    }
   ],
   "source": [
    "p1 = sentence_prob('He have to take the medicine.', P=P_mle)\n",
    "p2 = sentence_prob('He has to take the medicine.', P=P_mle)\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: -29.30948724296647; \n",
      "p2: -26.62036562628031\n"
     ]
    }
   ],
   "source": [
    "p1 = sentence_prob('He have to take the medicine.', P=P_mle)\n",
    "p2 = sentence_prob('He has to take the medicine.', P=P_mle)\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks good so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this model heavily depends on how large your dataset is.   \n",
    "If you give it the word or gram not existing in the corpus, the probability of it will be $0$, which causes problems during the calculation (because you can't divide numbers by $0$).  \n",
    "\n",
    "Try this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cdb52f75632d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"He has to eat medicine.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mP_mle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-f9f57029a6d6>\u001b[0m in \u001b[0;36msentence_prob\u001b[0;34m(sentence, P)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print(words[i+1] + ' | ' + words[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#print(log(P_mle(words[i+1], words[i])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "sentence_prob(\"He has to eat medicine.\", P=P_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? Now you get the problem.  \n",
    "Surely you can expand your dataset to decrease the number of unseen phrases, but it's impossible to cover all phrases in the world.  \n",
    "Here the smoothing technique comes to rescue!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - smoothing\n",
    "\n",
    "A naïve solution is to set the default frequency as `1`.  \n",
    "For example, if the original frequency is\n",
    "\n",
    "> hello: 2  \n",
    "> world: 1  \n",
    "> empty: 0  \n",
    "> null: 0  \n",
    "\n",
    ", after defaulting the frequency to 1, it would be\n",
    "\n",
    "> hello: 3  \n",
    "> world: 2  \n",
    "> empty: <font color=\"red\">**1**</font><br/>\n",
    "> null: <font color=\"red\">**1**</font><br/>\n",
    "\n",
    "\n",
    "And if the frequency is all added by 1, the probability of bigram will now be  \n",
    "$$ P_{add1} = \\frac{Freq(w_{i-1}, w_i)+1}{Freq(w_{i-1})+N}$$\n",
    ", where $N = count\\:of\\:all\\:tokens$ .  \n",
    "\n",
    "This is called the **Add-1 Estimation**, or also **Laplace Smoothing**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Implement the Laplace smoothing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_mle(word: str, pre_word: str = None):\n",
    "    \"\"\" [TODO] Return the P(word|pre_word), which should be a float \"\"\"\n",
    "    text = pre_word+' '+word\n",
    "    return bi_freq[text]/uni_freq[pre_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_add1(word: str, pre_word: str):\n",
    "    \"\"\" [TODO] Return the P(word|pre_word) with Laplace smoothing. \"\"\"\n",
    "    text = pre_word+' '+word\n",
    "    return (bi_freq[text]+1) / (uni_freq[pre_word]+len(uni_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 1.423224172039338e-05; \n",
      "p2: 2.8266553600452264e-05\n"
     ]
    }
   ],
   "source": [
    "# myself\n",
    "p1 = P_add1('medicine', 'eat')\n",
    "p2 = P_add1('medicine', 'take')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 1.423224172039338e-05; \n",
      "p2: 2.8266553600452264e-05\n"
     ]
    }
   ],
   "source": [
    "p1 = P_add1('medicine', 'eat')\n",
    "p2 = P_add1('medicine', 'take')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output:</font> `p1` shouldn't throw any error, and `p1` should be **smaller** than `p2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Add-1 estimation is a really naive method and doesn't always produce good results.  \n",
    "There are better smooting techiniques like <a href=\"https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation\">Good-Turing Estimation</a> or <a href=\"https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing\">Kneser-Ney Smoothing</a>. Feel free to give it a try!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your LM\n",
    "\n",
    "Seems that you've done a great job! Let's give it some tests to see if it works as expected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He have to take the medicine. -40.5645650850356\n",
      "He has to eat the medicine. -44.812473116633655\n",
      "He has to take the medicine. -38.7614966891868\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "  'He have to take the medicine.',\n",
    "  'He has to eat the medicine.',\n",
    "  'He has to take the medicine.'\n",
    "]\n",
    "\n",
    "for sent in test_cases:\n",
    "  print(sent, sentence_prob(sent, P=P_add1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output:</font> The last sentence should return the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is looking to a job. -41.658563026081836\n",
      "He is looking for a job. -40.50484303264153\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "  'He is looking to a job.',\n",
    "  'He is looking for a job.'\n",
    "]\n",
    "\n",
    "for sent in test_cases:\n",
    "  print(sent, sentence_prob(sent, P=P_add1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Expected output:</font> The second sentence should return a higher probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think about it\n",
    "Think about the following questions. **TA will ask your answers during the demo.**  \n",
    "\n",
    "1. What would happen if you use MLE Estimation to compare two sentences with different lengthes?  \n",
    "   Examples. (1) *He school nice and happy.* and (2) *He went to school yesterday and had a nice time there.*  \n",
    "\n",
    "2. According to Q1, Do you think MLE model is fair? If so, why? If not, how could you improve this? (Just think about it; no need to actually implement it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is looking for a job but I can not get it. -93.75761559638221\n",
      "He is looking for a job. -40.50484303264153\n",
      "\n",
      "\n",
      "He is looking up at the numbers of the houses. -67.49476439206863\n",
      "He is looking up. -27.053991314917603\n"
     ]
    }
   ],
   "source": [
    "# dict not have\n",
    "test_cases = [\n",
    "  'He is looking for a job but I can not get it.',\n",
    "  'He is looking for a job.'\n",
    "]\n",
    "\n",
    "for sent in test_cases:\n",
    "  print(sent, sentence_prob(sent, P=P_add1))\n",
    "print('\\n')\n",
    "test_cases = [\n",
    "  'He is looking up at the numbers of the houses.',\n",
    "  'He is looking up.'\n",
    "]\n",
    "for sent in test_cases:\n",
    "  print(sent, sentence_prob(sent, P=P_add1)) # mutli ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulation!** You've successfully built your own language model.  \n",
    "You should have caught a glimpse of LM's power despite its plainness.  \n",
    "Feel free to play around with your LM and improve it, like  \n",
    "1. using a larger dataset, \n",
    "2. applying better smoothing technique, or \n",
    "3. considering longer gram dependency (3- to 5-gram are resonable length).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TA's note\n",
    "\n",
    "\n",
    "Remember to <b><a href=\"https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit?usp=sharing\">make an appoiment with TA</a> to demo/explain your implementation <u>before <font color=\"red\">10/7 15:30</font></u></b> .  \n",
    "You should also save your file as {student_id}.ipynb and submit it to <a href=\"https://eeclass.nthu.edu.tw/course/homework/2384\">eeclass</a> .  \n",
    "**Late submission is not allowed.**  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e56d558c252a4887cb9bc110bf182e2aa9946109639baab21a0ce4014d1d01c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
