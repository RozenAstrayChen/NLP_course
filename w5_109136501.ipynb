{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9uBeehyAuXY"
   },
   "source": [
    "<b>This notebook is just a tutorial for you to get familiar with skip-gram and MapReduce.  \n",
    "<font color=\"red\">You don't need to hand in this notebook</font>, so feel free to jump to [Requirement section](#Assignment-Requirement) and directly work on your `mapper.py` and `reducer.py` if you already have the idea of how to do so.</b>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "galmjCj6M0b4"
   },
   "source": [
    "# Week 05: Skip-gram and MapReduce\n",
    "\n",
    "In previous assignments, you have known the concept of ngrams and how to generate them.  \n",
    "This week, we are introducing another gram type, called *skip-gram*, to you.  \n",
    "Also, we are going to calculate it on a large dataset, so you'll have to process it with the MapReduce technique.  \n",
    "\n",
    "So, first thing first: what is skip-gram?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLUfI2ZJGo2k"
   },
   "source": [
    "## Skip-gram\n",
    "\n",
    "<i>\\[S\\]kip-grams are a generalization of n-grams in which the components (typically words) need not be consecutive in the text under consideration, but may leave gaps that are skipped over.  - from [Wikipedia](https://en.wikipedia.org/wiki/N-gram#Skip-gram)</i>  \n",
    "\n",
    "That is, skip-gram is actually the same as ngram, but allowed to skip some words in between.  \n",
    "In the sentence <i>\"Strong winds blew roofs away\"</i>, two of its bigrams are <i>\"winds blew\"</i> and <i>\"blew roofs\"</i>, while <i>\"blew away\"</i> is one of the skipgrams with distance 2, since it skipped one word <i>\"roofs\"</i> .  \n",
    "As you can see, skipgram is able to capture the phrase seperated by other words.  \n",
    "\n",
    "Now consider another sentence\n",
    "\n",
    "> \"Skip-gram is used to predict the context word for a given target word\".\n",
    "\n",
    "With a pivot word *predict*, all of its skip-grams within distance 5 are as below:\n",
    "```\n",
    ".------------------------------------------------------------------------------.\n",
    "| distance || -5 |     -4    | -3 |  -2  | -1 |  1  |    2    |  3   |  4  | 5 |\n",
    "|----------||----|-----------|----|------|----|-----|---------|------|-----|---|\n",
    "| predict  || -  | Skip-gram | is | used | to | the | context | word | for | a |\n",
    "'------------------------------------------------------------------------------'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWl4bonyAuXb"
   },
   "source": [
    "<a name=\"Practice\"></a>\n",
    "### Practice: Distance table of skip-gram\n",
    "\n",
    "Now, let's practice!  \n",
    "Given a sentence <i>\"Skip-gram is used to predict the context word for a given target word\"</i>, <u>**output all of its skip-gram with distance between -3 to 3</u> and show the result in a table**.  \n",
    "\n",
    "**Example**\n",
    "```\n",
    "distance      -3            -2            -1            1             2             3             \n",
    "--------------------------------------------------------------------------------------------\n",
    "Skip-gram     -             -             -             is            used          to            \n",
    "is            -             -             Skip-gram     used          to            predict       \n",
    "used          -             Skip-gram     is            to            predict       the           \n",
    "to            Skip-gram     is            used          predict       the           context       \n",
    "predict       is            used          to            the           context       word          \n",
    "the           used          to            predict       context       word          for           \n",
    "context       to            predict       the           word          for           a             \n",
    "word          predict       the           context       for           a             given         \n",
    "for           the           context       word          a             given         target        \n",
    "a             context       word          for           given         target        word          \n",
    "given         word          for           a             target        word          -             \n",
    "target        for           a             given         word          -             -             \n",
    "word          a             given         target        -             -             -             \n",
    "```\n",
    "\n",
    "\\*Hint: Try to get the skip-grams for a single word first if you have trouble generating them all at once. \n",
    "```\n",
    "(predict, is, -3)\n",
    "(predict, used, -2)\n",
    "(predict, to, -1)\n",
    "(predict, the, 1)\n",
    "(predict, context, 2)\n",
    "(predict, word, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "T9ZFdmNcAuXb",
    "outputId": "70902117-7377-40e4-f937-66e1dca852ea"
   },
   "outputs": [],
   "source": [
    "#tokens = \"Skip-gram is used to predict the context word for a given target word\".split()\n",
    "def skip_gram(tokens):\n",
    "    token_length = len(tokens)\n",
    "    f_max_dis = 5; p_max_dis = -5\n",
    "    gram = []\n",
    "\n",
    "    for idx in range(token_length):\n",
    "        # pervious\n",
    "        for i in range(-1, p_max_dis-1, -1):\n",
    "            if idx+i >= 0:\n",
    "                gram.append((tokens[idx], tokens[idx+i], i, 1))\n",
    "        for i in range(1, f_max_dis+1):\n",
    "            if idx+i < token_length:\n",
    "                gram.append((tokens[idx], tokens[idx+i], i, 1))\n",
    "\n",
    "    return gram\n",
    "#skip_gram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "distance      -3            -2            -1            1             2             3             \n",
    "--------------------------------------------------------------------------------------------\n",
    "Skip-gram     -             -             -             is            used          to            \n",
    "is            -             -             Skip-gram     used          to            predict       \n",
    "used          -             Skip-gram     is            to            predict       the           \n",
    "to            Skip-gram     is            used          predict       the           context       \n",
    "predict       is            used          to            the           context       word          \n",
    "the           used          to            predict       context       word          for           \n",
    "context       to            predict       the           word          for           a             \n",
    "word          predict       the           context       for           a             given         \n",
    "for           the           context       word          a             given         target        \n",
    "a             context       word          for           given         target        word          \n",
    "given         word          for           a             target        word          -             \n",
    "target        for           a             given         word          -             -             \n",
    "word          a             given         target        -             -             -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_r1nxWlGAuXd"
   },
   "source": [
    "## MapReduce\n",
    "\n",
    "<i>MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster. - from [Wikipedia](https://en.wikipedia.org/wiki/MapReduce)</i> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxEq1CzVAuXd"
   },
   "source": [
    "### Why MapReduce?\n",
    "\n",
    "Imagine that you are working on a pretty large dataset, say all pages on Wikipedia (whose size has already reached 94GB in 2013).  \n",
    "Most likely you are not able to process the whole corpus in the memory or on a single computer. Even a simple frequency counter would be challenging under such a huge data size.  \n",
    "To deal with this, Google proposed a big-data processing model called MapReduce, and it has been implemented and supported by many distributed computing systems, such as Apache Hadoop.  \n",
    "The core concept of MapReduce is to **split, apply and then combine**, so that each data segment can be handled separately.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9o0eEzCSnnv"
   },
   "source": [
    "### Mapper-Shuffler-Reducer\n",
    "\n",
    "![](https://www.todaysoftmag.com/images/articles/tsm33/large/a11.png)\n",
    "<small><i> - image source: [Today Software Magazine](https://www.todaysoftmag.com/article/1358/hadoop-mapreduce-deep-diving-and-tuning)</i></small> \n",
    "\n",
    "As you can see in the picture:  \n",
    "First, the whole data is split into some smaller partitions, each partition able to be processed by an independant machine.  \n",
    "In this step, **mappers** will generate one or more key-value pair(s) that can easily be clustered.  \n",
    " - example: in a word counter, it would generate the word and the word's current count.  \n",
    "\n",
    "Then, we will **shuffle** and group all outputs from mappers.  \n",
    " - example: sort the output from mappers.  \n",
    "\n",
    "Lastly, we can combine the grouped values and **reduce** them into final results.  \n",
    " - example: calculate total frequency in each group.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNtZvD-uAuXd"
   },
   "source": [
    "## MapReduce for skip-gram\n",
    "\n",
    "Now, having the concepts of skip-gram and MapReduce in mind, it's time to put these all together: let's generate skip-gram table with MapReduce technique!  \n",
    "\n",
    "It may sound scary to some of you, so let's break it down first.  \n",
    "There are 3 steps to do, and each step is described as below:\n",
    "1. **Mapper**: Print all skip-gram with its distance infomation, and the current count of it.  \n",
    "   ```\n",
    "   a b -3 1\n",
    "   a c 3  1\n",
    "   c e -2 1\n",
    "   a c 1  1\n",
    "   a b -3 1\n",
    "   b d 2  1\n",
    "   ```\n",
    "2. **Shuffler**: Group all skipgrams by its text. This can be easily achieved with sorting.  \n",
    "   ```\n",
    "   a b -3 1\n",
    "   a b -3 1\n",
    "   a c 1  1\n",
    "   a c 3  1\n",
    "   b d 2  1\n",
    "   c e -2 1\n",
    "   ```\n",
    "3. **Reducer** :  \n",
    "   Since the results have been sorted in the previous step, we can easily calculate the frequency of each skip-gram with different distance.  \n",
    "   So we can know that the frequency of skipgram `a b` with distance $-3$ should be $1+1=2$, while other skip-grams' are all $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7pnB2AYaH2c"
   },
   "source": [
    "### Step 1: Mapper\n",
    "\n",
    "First, in the mapper we want to generate all skip-grams within distance $-5$ to $5$.  \n",
    "Remember that you've already done something similar in [previous Practice](#Practice)? Just modify it to MapReduce format!  \n",
    "\n",
    "Output: \n",
    " - `\"{pivot}\\t{word}\\t{distance}\\t{count}\"`  \n",
    "\n",
    "Example: \n",
    "```\n",
    "predict is  -3  1\n",
    "predict used    -2  1\n",
    "predict the -1  1\n",
    "predict the 1   1\n",
    "...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "j3PxzsHnGo2v"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "dvt8Q9D5Z0WV",
    "outputId": "cdf72dd3-1194-4a13-c9f4-815a20a4b0dd"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'wiki1G.txt')) as f:\n",
    "    for line in f:\n",
    "        #toekns = re.findall(r'\\S+', line)\n",
    "        tokens = re.sub(r'[^\\w\\s]',' ',line).lower().split()\n",
    "        grams = skip_gram(tokens) \n",
    "        break # for the sake of this practice, just test the first page now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(gram)\n",
    "#df.to_csv('skip_gram/mapper.csv', index=False)\n",
    "with open('skip_gram/mapper.tsv', 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    for gram in grams:\n",
    "        tsv_writer.writerow(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('sort -k1,3 < skip_gram/mapper.tsv > skip_gram/shuffler.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlEDNwlNdELJ"
   },
   "source": [
    "### Step 2: Shuffler\n",
    "\n",
    "All we need to do in the shuffler is sorting, so let's use the built-in command to do this for us!  \n",
    "\n",
    "Try this on your terminal/command prompt ;)  \n",
    "(You can get the sample input from [here](https://drive.google.com/drive/folders/1vKxr--sLd2J4kdsXUzJDBZdG3AmV4NGl?usp=sharing))\n",
    "\n",
    "**Unix**  \n",
    "```bash\n",
    "sort -k1,3 < mapper.sample.tsv\n",
    "```\n",
    "**Windows**\n",
    "```powershell\n",
    "type mapper.sample.tsv | sort\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KeVLicCdmgp"
   },
   "source": [
    "### Step 3: Reducer\n",
    "\n",
    "Since all the input should have been sorted in previous shuffler, the task of reducer is pretty simple: just count how many times the same gram appears, and then print the count out!\n",
    "\n",
    "Input: \n",
    " - `\"{pivot}\\t{word}\\t{distance}\\t{count}\"`\n",
    " - You can get a sample input file `shuffler.sample.tsv` from [here](https://drive.google.com/drive/folders/1vKxr--sLd2J4kdsXUzJDBZdG3AmV4NGl?usp=sharing)\n",
    "\n",
    "Output: \n",
    " - `\"{pivot}\\t{word}\\t{total_freq}\\t{-5}\\t{-4}\\t{-3}\\t{-2}\\t{-1}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\"`\n",
    " - The first two column are the skipgram; the third column is the sum of total frequency; column 4\\~13 are the frequency with distance -5\\~5, without 0.\n",
    "\n",
    "Example:\n",
    " - `arouse  open    4       0       0       3       0       0       0       0       0       0       1`\n",
    "\n",
    "Hints: \n",
    "1. Parse the input from shuffler\n",
    "2. Check if this is the same skipgram as the previous one\n",
    "3. If so, add the frequency according to its distance\n",
    "4. If not, output the previous skipgram data\n",
    "\n",
    "Note that you may NOT want to store all your counting results in a dict or any data structure.  \n",
    "Recall that one purpose of MapReduce is to prevent memory exhaustion. It loses its value if you end up storing it again.  \n",
    "Instead, <u>directly print it out or write it into a file</u> .  \n",
    "(Don't get me wrong: of course you can store some temporary data, but let's not store the whole result and then print it out at once, okay?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "-qU0k-Zod3V1",
    "outputId": "81ebdd88-d4c3-4463-c00f-6c3bf8816bce"
   },
   "outputs": [],
   "source": [
    "'''def generate_reducer(reducer_dict, skip_list):\n",
    "\n",
    "    if (skip_list[0], skip_list[1]) not in reducer_dict.keys():\n",
    "        reducer_dict[(skip_list[0], skip_list[1])] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    if int(skip_list[2]) == 5:\n",
    "        reducer_dict[(skip_list[0], skip_list[1])][10] += int(skip_list[3])\n",
    "    if int(skip_list[2]) == 4:\n",
    "        reducer_dict[(skip_list[0], skip_list[1])][9] += int(skip_list[3])\n",
    "    if int(skip_list[2]) == 3:\n",
    "        reducer_dict[(skip_list[0], skip_list[1])][8] += int(skip_list[3])\n",
    "    if int(skip_list[2]) == 2:\n",
    "        reducer_dict[(skip_list[0], skip_list[1])][7] += int(skip_list[3])\n",
    "    if int(skip_list[2]) == 1:\n",
    "        reducer_dict[(skip_list[0], skip_list[1])][6] += int(skip_list[3])\n",
    "    if int(skip_list[2]) == -1:\n",
    "        reducer_dict[(skip_list[0], skip_list[1])][5] += int(skip_list[3])\n",
    "    if int(skip_list[2]) == -2:\n",
    "        reducer_dict[(skip_list[0], skip_list[1])][4] += int(skip_list[3])\n",
    "    if int(skip_list[2]) == -3:\n",
    "        reducer_dict[(skip_list[0], skip_list[1])][3] += int(skip_list[3])\n",
    "    if int(skip_list[2]) == -4:\n",
    "        reducer_dict[(skip_list[0], skip_list[1])][2] += int(skip_list[3])\n",
    "    if int(skip_list[2]) == -5:\n",
    "        reducer_dict[(skip_list[0], skip_list[1])][1] += int(skip_list[3])\n",
    "        \n",
    "    reducer_dict[(skip_list[0], skip_list[1])][0] += int(skip_list[3])'''\n",
    "    \n",
    "def generate_reducer2(reducer_list, skip_list, index=0, overwrite=False):\n",
    "    if overwrite == False:\n",
    "        # generate new\n",
    "        init_list = [skip_list[0], skip_list[1], int(skip_list[3]), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] # 3..12\n",
    "        \n",
    "        if int(skip_list[2])>0:\n",
    "            init_list[7+int(skip_list[2])] += int(skip_list[3])\n",
    "        else:\n",
    "            init_list[8+int(skip_list[2])] += int(skip_list[3])\n",
    "        #init_list[7+int(skip_list[2])]+=int(skip_list[3]) if int(skip_list[2])>0 else init_list[8+int(skip_list[2])]+=int(skip_list[3])\n",
    "        reducer_list.append(init_list)\n",
    "        \n",
    "    elif overwrite == True:\n",
    "        reducer_list[index][2] += int(skip_list[3])\n",
    "        if int(skip_list[2])>0:\n",
    "            reducer_list[index][7+int(skip_list[2])]+=int(skip_list[3]) \n",
    "        else:\n",
    "            reducer_list[index][8+int(skip_list[2])]+=int(skip_list[3]) \n",
    "        #reducer_list[index][7+int(skip_list[2])]+=int(skip_list[3]) if int(skip_list[2])>0 else reducer_list[index][8+int(skip_list[2])]+=int(skip_list[3])\n",
    "    return reducer_list\n",
    "    \n",
    "pviot = ''; word ='';reducer_dict = {}; reducer_list = []; index = 0; count=0\n",
    "with open(os.path.join('skip_gram', 'shuffler.tsv')) as f:\n",
    "    # 1) Parse the input from shuffler\n",
    "    # 2) Check if this is the same skipgram\n",
    "    # 3) If so, add the frequency according to its distance\n",
    "    # 4) If not, output the previous skipgram data\n",
    "    for line in f:\n",
    "        skip_list = line.split()\n",
    "        '''\n",
    "        generate_reducer(reducer_dict, skip_list)\n",
    "        '''\n",
    "        if pviot != skip_list[0]: #pivot not same\n",
    "            pviot = skip_list[0]; word = skip_list[1]\n",
    "            reducer_list = generate_reducer2(reducer_list, skip_list)\n",
    "            index += 1\n",
    "\n",
    "        elif (pviot == skip_list[0]) and (word != skip_list[1]):\n",
    "            word = skip_list[1]\n",
    "            reducer_list = generate_reducer2(reducer_list, skip_list)\n",
    "            index += 1          \n",
    "        elif (pviot == skip_list[0]) and (word == skip_list[1]):\n",
    "            reducer_list = generate_reducer2(reducer_list, skip_list, index-1, True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('skip_gram/reducer.tsv', 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    for reducer in reducer_list:\n",
    "        tsv_writer.writerow(reducer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdtatMISGo2x"
   },
   "source": [
    "### Step 4: Combine them together!  \n",
    "\n",
    "Now you can move your code above into mapper.py and reducer.py (with some tiny modifications, of course), and this is your assignment this week!   \n",
    "See below for detailed requirement description.  \n",
    "\n",
    "**Hints: What should I modify in my mapper and reducer?**  \n",
    "\n",
    "1. Receive/pass data from standard I/O, rather than the file (We've already done this for you)\n",
    "2. Process with the whole dataset, rather than only the first line\n",
    "\n",
    "That's it!  \n",
    "\n",
    "The processing takes some times (~1hr w/o parallel computing), so go enjoy some coffee or movies (or sleep) during the waiting time ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfBCNmkl6ww8"
   },
   "source": [
    "<a name=\"Assignment-Requirement\"></a>\n",
    "## Assignment Requirement \n",
    "\n",
    "1. You need to implement the `mapper.py` and `reducer.py` to calculate the skip-gram table.\n",
    "\n",
    "2. In `mapper.py`, you need to generate skipgrams with distance within -5 to 5 (inclusive).  \n",
    "   - Input: Pure text file (`wiki1G.txt`) with each line as a wikipage.\n",
    "   - Output: `\"{pivot}\\t{word}\\t{distance}\\t{count}\"`\n",
    "   - Example: \n",
    "     ```\n",
    "     predict is  -3  1\n",
    "     predict used    -2  1\n",
    "     predict the -1  1\n",
    "     predict the 1   1\n",
    "     ...\n",
    "     ```\n",
    "   - Sample output: `mapper.sample.tsv` (Find it [here](https://drive.google.com/drive/folders/1vKxr--sLd2J4kdsXUzJDBZdG3AmV4NGl?usp=sharing); no need to be exactly the same)\n",
    "\n",
    "3. In `reducer.py`, you have to collect the output from the shuffler (`sort`) and generate the skip-gram table.\n",
    "   - Input: `\"{pivot}\\t{word}\\t{distance}\\t{count}\"`\n",
    "   - Output: \n",
    "     - `\"{pivot}\\t{word}\\t{total}\\t{-5}\\t{-4}\\t{-3}\\t{-2}\\t{-1}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\"`\n",
    "     - The first two column are the skipgram; the third column is the sum of total frequency; column 4\\~13 are the frequency with distance -5\\~5, without 0.\n",
    "   - Example:\n",
    "     ```\n",
    "     arouse  of      1       0       0       0       0       0       0       0       1       0       0\n",
    "     arouse  open    4       0       0       3       0       0       0       0       0       0       1\n",
    "     arouse  so      2       0       1       0       0       0       0       0       0       1       0\n",
    "     arouse  sufficiently    1       0       1       0       0       0       0       0       0       0       0\n",
    "     ...\n",
    "     ```\n",
    "   - Sample output: `reducer.sample.tsv` (Find it [here](https://drive.google.com/drive/folders/1vKxr--sLd2J4kdsXUzJDBZdG3AmV4NGl?usp=sharing); no need to be exactly the same)\n",
    "\n",
    "4. Concate your MapReduce procedure and generate the skip-gram on wiki1G dataset\n",
    "   - Unix: \n",
    "     - Use the [local map-reduce tool](https://github.com/dspp779/local-mapreduce) (faster),\n",
    "     - or run it directly: `python mapper.py < wiki1G.txt | sort -k1,2 -k3n | python reducer.py > skipgram.tsv` (slower)\n",
    "   - Windows: \n",
    "     - CMD: `python mapper.py < wiki1G.txt | sort | python reducer.py > skipgram.tsv`\n",
    "     - PS: `type wiki1G.txt | python mapper.py | sort | python reducer.py > skipgram.tsv`\n",
    "     - or the bash environment you installed last week.  \n",
    "   - See [Appendix](#built-in-command) if you want to know what these commands mean\n",
    "\n",
    "During the demo, you need to \n",
    "\n",
    "1. show us your skip-gram result on the given dataset, and\n",
    "2. explain your implementation in `mapper.py` and `reducer.py`.  \n",
    "\n",
    "Note that the final result would be a large file (~6 GB), so **you may want to show it with `more` or `less` command**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfEQMmPDAuXe"
   },
   "source": [
    "## TA's note\n",
    "\n",
    "Congratulations! You've learned how to calculate skipgram frequency and to deal with a huge dataset with MapReduce technique.  \n",
    "\n",
    "Remember to <b><a href=\"https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit?usp=sharing\">make an appoiment with TA</a> to demo/explain your implementation <u>before <font color=\"red\">10/21 15:30</font></u></b> .  \n",
    "You should also submit your `mapper.py` and `reducer.py` to <a href=\"https://eeclass.nthu.edu.tw/course/homework/3285\">eeclass</a> ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd60PpiuSqxU"
   },
   "source": [
    "<a name=\"built-in-command\"></a>\n",
    "## Appendix: useful built-in commands\n",
    "\n",
    "Several built-in commands are very useful in the MapReducer procedure.  \n",
    "Here we introduce `cat` and `type`, `<` and `>`, `sort`, and pipe `|`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwtH-e3qO93D"
   },
   "source": [
    "### cat (on Unix)\n",
    "`cat` command, which is definitly not indicating some cute creatures (*meow~*), is the abbreviation of `concatenate`. ([doc](https://man7.org/linux/man-pages/man1/cat.1.html))   \n",
    "\n",
    "When you `cat` a file, it means you want to print the content from a file (or some files) to standard output.  \n",
    "Now open your bash and test the command below!  \n",
    "```bash\n",
    "cat file.txt\n",
    "```\n",
    "\n",
    "You should see something like this: \n",
    "\n",
    "![picture](https://i.imgur.com/Z9shOYQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA8C7bh1O_N8"
   },
   "source": [
    "### type (on Windows)\n",
    "`type` command works exactly the same as `cat` on Unix, but without its cute nickname (Shame on you, Windows). ([doc](https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/type))  \n",
    "\n",
    "Similarly, if you `type` a file, it means you print the content from a file (or some files) to standard output.  \n",
    "\n",
    "```powershell\n",
    "type file.txt\n",
    "```\n",
    "\n",
    "You should see something like this:  \n",
    "![](https://i.imgur.com/5WFhxkq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `>`? `<`? `>///<`? \n",
    "\n",
    "`<` and `>` are the I/O redirections.  \n",
    "`program < filename` means that you want to redirect the input from a file to a program, while `program > filename` means that you want to redirect the output of a program to that file.  \n",
    "\n",
    "For example,\n",
    "```bash\n",
    "echo \"hello world\" > greet.txt\n",
    "```\n",
    "writes the string \"hello world\" into a file `greet.txt`.  \n",
    "\n",
    "On the other hand, \n",
    "```bash\n",
    "head < greet.txt\n",
    "```\n",
    "makes `head` receive the content from `greet.txt`, so it will print out the string in `greet.txt`.  \n",
    "![](https://i.imgur.com/swxv8LG.png)\n",
    "<small>p.s. `>///<` is just a joke. Don't take it seriously.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVM5KXA9G6N1"
   },
   "source": [
    "### sort\n",
    "\n",
    "As its name suggests, `sort` sorts the data that it receives. (doc on [Linux](https://man7.org/linux/man-pages/man1/sort.1.html) and on [Windows](https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/sort))  \n",
    "Try this:\n",
    "```\n",
    "sort sample.txt\n",
    "```\n",
    "You can see that the content has been sorted before printed onto your screen.  \n",
    "![](https://i.imgur.com/QFEq3Tc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeB0FL2INW57"
   },
   "source": [
    "### Pipe `|`\n",
    "\n",
    "Pipe passes the output from previous program to the next program.  \n",
    "For example, \n",
    "```bash\n",
    "python program.py | sort\n",
    "```\n",
    "will pass the output of `program.py` to `sort` command.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rLUfI2ZJGo2k"
   ],
   "name": "Week05_skipgram.sample.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "0e56d558c252a4887cb9bc110bf182e2aa9946109639baab21a0ce4014d1d01c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
